---
title: "qwen3について詳しく教えて"
source: "https://www.perplexity.ai/search/qwen3nituitexiang-sikujiao-ete-MacLHh5hRGy41m102C8XlA"
author:
  - "[[Perplexity AI]]"
published:
created: 2025-05-01
description: "Qwen3は、2025年4月29日にAlibaba Cloudからリリースされた最新の大規模言語モデル(LLM)ファミリーです。特徴的なのは、問題解決のためのハイブリッドアプローチを導入しており、複雑な問題に対して段階的な推論を行う「思考モード」と、簡単な質問に迅速に回答する「非思考モード」の両方をサポートしていること..."
tags:
  - "clippings"
---
ayumu\_harata

4 時間前

qwen3について詳しく教えて[note](https://note.com/npaka/n/n43abd5843fe7)

[

Qwen3 の概要｜npaka - note

](https://note.com/npaka/n/n43abd5843fe7)[Note（ノート）](https://note.com/makokon/n/n6c4e929949c8)

[

Qwen3シリーズの発表。小規模オープンウェイトながらなかなかの...

](https://note.com/makokon/n/n6c4e929949c8)[AIじゃないよ](https://notai.jp/qwen3/)

[

Deepseek R1 を超えた中国アリババのオープンソース推論モデル

](https://notai.jp/qwen3/)

+49 ソース

## Qwen3の詳細な使い方ガイド

Qwen3は、2025年4月29日にAlibaba Cloudからリリースされた最新の大規模言語モデル(LLM)ファミリーです [2](https://note.com/makokon/n/n6c4e929949c8) [3](https://notai.jp/qwen3/) 。特徴的なのは、問題解決のためのハイブリッドアプローチを導入しており、複雑な問題に対して段階的な推論を行う「思考モード」と、簡単な質問に迅速に回答する「非思考モード」の両方をサポートしていることです [2](https://note.com/makokon/n/n6c4e929949c8) [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/) 。

Qwen3シリーズは以下の8つのモデルから構成されています：

- : Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, Qwen3-8B, Qwen3-14B, Qwen3-32B
- : Qwen3-30B-A3B (30B total / 3B active), Qwen3-235B-A22B (235B total / 22B active) [3](https://notai.jp/qwen3/)

特筆すべき点として、119の言語と方言をサポートする多言語機能、最大32Kトークンのコンテキスト長、そして強化されたコーディングとエージェント機能を備えています [2](https://note.com/makokon/n/n6c4e929949c8) [11](https://note.com/kyutaro15/n/na36686e5aca4) 。さらに、商用利用や改変・複製・再配布が可能なApache 2.0ライセンスで提供されている点も大きな特徴です [3](https://notai.jp/qwen3/) [7](https://weel.co.jp/media/tech/qwen-3/) 。

1. Ollamaの公式サイト(ollama.com)からインストーラーをダウンロード
2. インストーラーを実行してOllamaをインストール
3. コマンドプロンプトを開き、以下のコマンドを実行：
	```
	text
	ollama run qwen3
	```
1. LMStudioをインストールして起動
2. コマンドラインツール「lms」をブートストラップ:
	- macOSやLinux: `~/.lmstudio/bin/lms bootstrap`
	- Windows: `cmd /c %USERPROFILE%/.lmstudio/bin/lms.exe bootstrap`
3. 利用可能なQwen3モデルをダウンロード:
	```
	text
	lms get qwen3-1.7b
	```

Qwen3は、Hugging Face、ModelScope、Kaggleでも公開されており、これらのプラットフォームを通じてアクセス可能です [6](https://zenn.dev/fujiyuu75/articles/899c7816bacc00) [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/) 。

1. サーバーの起動:
	```
	text
	lms server start
	```
2. 使用が終わったら停止:
	```
	text
	lms server stop
	```

```
python
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
```

1. プロンプトを入力して送信
2. モデルからの応答を受け取る
3. 必要に応じて思考モードを切り替える（詳細は後述）

```
python
from openai import OpenAI

# ローカルサーバーに接続
client = OpenAI(base_url="http://localhost:1234/v1",
api_key="lm-studio")

completion = client.chat.completions.create(
    model="qwen3-1.7b",
    messages=[
        {"role": "system", "content": "Always answer in rhymes."},
        {"role": "user", "content": "Introduce yourself."}
    ],
    temperature=0.7
)

print(completion.choices[0].message)
```

Qwen3の特徴的な機能は、ユーザーが制御可能な2つの異なるモードです：

- : モデルは内部で段階的な推論を行い、複雑なタスクでのパフォーマンスを向上させます
- : モデルは明示的な内部推論フェーズなしで直接応答を生成し、単純なクエリに対して速度と計算コストを最適化します [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/)

プロンプトやシステムメッセージ内に `/think` や `/no_think` を挿入することで、対話ごとに思考モードを動的に変更できます [6](https://zenn.dev/fujiyuu75/articles/899c7816bacc00) 。

```
text
"messages": [
    {"role": "user", "content": "'Good morning' をスペイン語に翻訳してください。 /no_think"}
]
```

Qwen3はツール呼び出し機能に優れています [1](https://note.com/npaka/n/n43abd5843fe7) 。Qwen-Agentを使用することでツール連携のコーディングを大幅に簡素化できます：

```
python
from qwen_agent.agents import Assistant

llm_cfg = {
    'model': 'Qwen3-30B-A3B',
    'model_server': 'http://localhost:8000/v1',
    'api_key': 'EMPTY',
}

tools = [
    {'mcpServers': {
        'time': {
            'command': 'uvx',
            'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
        }
    }},
    'code_interpreter',
]

bot = Assistant(llm=llm_cfg, function_list=tools)
```

Qwen3は強化されたコーディング機能を持ち、プログラミングアシスタントとして非常に有用です：

```
text
"messages": [
    {"role": "user", "content": "3つの数値の最大値を見つけるPython関数を作成してください。"}
]
```

応答例：

```
python
def find_max(a, b, c):
    if a >= b and a >= c:
        return a
    elif b >= a and b >= c:
        return b
    else:
        return c
```

数学的推論能力が高く、複雑な問題も解決できます：

```
text
"messages": [
    {"role": "user", "content": "x^2 - 5x + 6 = 0 を解いてください"}
]
```

119の言語と方言をサポートしており、翻訳などの多言語タスクに最適です：

```
text
"messages": [
    {"role": "user", "content": "'Good morning' をスペイン語に翻訳してください。"}
]
```

応答例： `"Buenos días"` [14](https://apidog.com/jp/blog/use-qwen3-apis-free/)

- モデルサイズと利用可能なハードウェアリソースを確認
- 小さいモデル（Qwen3-0.6B, Qwen3-1.7B）から始めて、システム性能を測定
- MoEモデル実行時のGPU設定を適切に行う [5](https://zenn.dev/chitako/articles/5f1699fcbef6f9)

モデルサイズとハードウェア能力に応じてパフォーマンスが変化します：

- 思考モードと非思考モードを適切に使い分ける
- シンプルなタスクには小さいモデルを使用する
- 複雑なタスクには大きなモデルを使用する [5](https://zenn.dev/chitako/articles/5f1699fcbef6f9) [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/)

Qwen3はApache 2.0ライセンスで提供されており、商用利用や改変・複製・再配布が可能です [3](https://notai.jp/qwen3/) 。また、ローカル実行が可能なため、データプライバシーの懸念を軽減できます [7](https://weel.co.jp/media/tech/qwen-3/) [10](https://chatgpt-enterprise.jp/blog/qwen3/) 。

特に企業向けには、インターネットに接続されていないオンプレミス環境での利用も可能で、機密情報を外部に送信することなく利用できる点が重要です [10](https://chatgpt-enterprise.jp/blog/qwen3/) 。

- : OpenAIのo1やo3-miniなどと並ぶ性能 [2](https://note.com/makokon/n/n6c4e929949c8)
- : 一部のベンチマークでGPT-4oを上回ると発表 [2](https://note.com/makokon/n/n6c4e929949c8)
- : Deepseek-R1やOpenAI-o1などの競合モデルを超える性能を示している [3](https://notai.jp/qwen3/)
- Qwen3-1.7B ≈ Qwen2.5-3B
- Qwen3-4B ≈ Qwen2.5-7B（一部の側面でQwen2.5-72B-Instructに匹敵）
- Qwen3-8B ≈ Qwen2.5-14B
- Qwen3-14B ≈ Qwen2.5-32B
- Qwen3-32B ≈ Qwen2.5-72B [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/)
- オープンソースでの提供（Apache 2.0ライセンス）
- ハイブリッド思考モードのサポート
- MoEアーキテクチャによる効率性（パラメータの約10%のみを活性化） [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/)
1. : タスクの複雑さに応じて思考モードと非思考モードを使い分ける [13](https://apidog.com/jp/blog/run-qwen-3-locally-jp-2/)
2. : 小さいモデルでプロトタイピングし、必要に応じて大きなモデルにスケールアップする [10](https://chatgpt-enterprise.jp/blog/qwen3/)
3. : 明確で具体的な指示を与え、必要に応じて思考モードを明示的に指定する
4. : Qwen-Agentを使ってツール連携を実現し、実用性を高める [1](https://note.com/npaka/n/n43abd5843fe7) [6](https://zenn.dev/fujiyuu75/articles/899c7816bacc00)
5. : 119の言語と方言をサポートしているため、多言語プロジェクトに積極的に活用する [2](https://note.com/makokon/n/n6c4e929949c8) [11](https://note.com/kyutaro15/n/na36686e5aca4)

Qwen3は、思考モードと非思考モードを組み合わせたハイブリッドアプローチや強力な多言語サポート、そしてオープンソースでの提供など、多くの強みを持つ最新のLLMファミリーです。用途に合わせて適切なサイズのモデルを選択し、その特性を理解して活用することで、様々なタスクを効率的に処理することができます。

Qwen3のMoEモデルとDenseモーの違いは

Qwen3のMoEモデルとDenseモーの違いは[news.yahoo.co](https://news.yahoo.co.jp/articles/286032b2cc2b5fcc31fd90b1f7e7ee52dee6106f)

[

第2のDeepSeekショック？ オープンな中国LLM「Qwen3」シリーズ...

](https://news.yahoo.co.jp/articles/286032b2cc2b5fcc31fd90b1f7e7ee52dee6106f)[TechTarget Japan](https://www.itmedia.co.jp/aiplus/articles/2504/29/news087.html)

[

第2のDeepSeekショック？ オープンな中国LLM「Qwen3」シリーズ...

](https://www.itmedia.co.jp/aiplus/articles/2504/29/news087.html)[zenn](https://zenn.dev/taku_sid/articles/20250429_qwen3_explanation)

[

うさぎでもわかるQwen3 最新の大規模言語モデルを徹底解説 - Zenn

](https://zenn.dev/taku_sid/articles/20250429_qwen3_explanation)

+17 ソース

では、現時点で画像理解能力が高いローカルllmのモデルは何ですか?

では、現時点で画像理解能力が高いローカルllmのモデルは何ですか?[Note（ノート）](https://note.com/catap_art3d/n/na046217a6246)

[

【推し】現在利用しているローカルLLM【日本語LLM】 - note

](https://note.com/catap_art3d/n/na046217a6246)[Note（ノート）](https://note.com/genaird/n/n98415f48cd95)

[

Qwen2-VL: ローカルで動作するビジョン・ランゲージ・モデル - note

](https://note.com/genaird/n/n98415f48cd95)[Note（ノート）](https://note.com/catap_art3d/n/n7131b2dcf246)

[

ローカルLLMの活用【画像生成編】｜Catapp-Art3D - note

](https://note.com/catap_art3d/n/n7131b2dcf246)

+17 ソース

現時点で画像理解能力が高いローカルLLMモデルとしては、以下のものが挙げられます：

- Xtuner氏（コミュニティ）が開発したVLM（Vision Language Model） [1](https://note.com/catap_art3d/n/na046217a6246)
- Meta llama3/CLIP/Llavaなどの複数のモデルを組み合わせて開発 [1](https://note.com/catap_art3d/n/na046217a6246)
- パラメータ規模：3.8B [1](https://note.com/catap_art3d/n/na046217a6246)
- `ollama pull llava-llama3` でインストール可能 [1](https://note.com/catap_art3d/n/na046217a6246)
- 「現状のVLMでは（画像生成に利用する場合に）一番優秀」と評価されている [1](https://note.com/catap_art3d/n/na046217a6246)
- 同じくXtuner氏が開発したVLM [1](https://note.com/catap_art3d/n/na046217a6246)
- Microsoft Phi3/CLIP/llavaなどの複数のモデルを利用 [1](https://note.com/catap_art3d/n/na046217a6246)
- パラメータ規模：3.8B [1](https://note.com/catap_art3d/n/na046217a6246)
- `ollama pull llava-phi3` でインストール可能 [1](https://note.com/catap_art3d/n/na046217a6246)
- llava-llama3と同程度の性能だが、より少ないGPUメモリで動作（GTX 1660ti 6GBでも実用的に動作） [1](https://note.com/catap_art3d/n/na046217a6246)
- Alibabaが2024年10月に発表したビジョン・ランゲージ・モデル [2](https://note.com/genaird/n/n98415f48cd95)
- 2B、7B、72Bの3つのモデルサイズがあり、用途に応じた選択が可能 [2](https://note.com/genaird/n/n98415f48cd95)
- 日本語を含むマルチリンガル対応 [2](https://note.com/genaird/n/n98415f48cd95)
- 動画解析機能（最大30フレーム）も搭載 [2](https://note.com/genaird/n/n98415f48cd95)
- PDF文書の画像データ解析などRAG（Retrieval-Augmented Generation）への応用例あり [2](https://note.com/genaird/n/n98415f48cd95)
- Meta社が開発した「Llama 3.2」の派生モデル [5](https://qrk.co.jp/llama_vision)
- 画像とテキストの両方を理解できるマルチモーダルモデル [5](https://qrk.co.jp/llama_vision)
- 画像キャプション生成、視覚質問応答、画像検索、画像編集など多様なタスクに対応 [5](https://qrk.co.jp/llama_vision)
- ollamaバージョン0.4以上で利用可能 [5](https://qrk.co.jp/llama_vision)

これらのモデルはいずれも高い画像理解能力を持ち、特にllava-llama3は画像生成用途において最も優秀とされています [1](https://note.com/catap_art3d/n/na046217a6246) 。また、llava-phi3は少ないGPUメモリでも動作する点が特徴的で、限られたハードウェア環境でも実用的に利用できます [1](https://note.com/catap_art3d/n/na046217a6246) 。

gpuメモリが12GBです。そのメモリで動作できる画像理解能力が高いローカルllmは？

gpuメモリが12GBです。そのメモリで動作できる画像理解能力が高いローカルllmは？[reddit.com](https://www.reddit.com/r/LocalLLaMA/comments/1765g70/from_no_gpu_to_a_3060_12gb_what_can_i_run/)

[

From no GPU to a 3060 12gb, what can I run?: r/LocalLLaMA - Reddit

](https://www.reddit.com/r/LocalLLaMA/comments/1765g70/from_no_gpu_to_a_3060_12gb_what_can_i_run/)[apxml.com](https://apxml.com/posts/gemma-3-gpu-requirements)

[

GPU System Requirements Guide for Gemma 3 Multimodal

](https://apxml.com/posts/gemma-3-gpu-requirements)[AI Advances](https://ai.gopubby.com/one-gpu-is-all-you-need-fine-tuning-12b-vision-language-models-locally-21438284dc66)

[

One GPU Is All You Need: Fine-Tuning 12B Vision-Language...

](https://ai.gopubby.com/one-gpu-is-all-you-need-fine-tuning-12b-vision-language-models-locally-21438284dc66)

+17 ソース

## 12GB GPUメモリで動作する画像理解能力の高いローカルLLM

12GB GPUメモリを持つ環境（RTX 3060など）で動作可能な画像理解能力の高いローカルLLMモデルには、以下のものがあります：

- ：画像-テキスト処理に必要なVRAMが7.8GBなので、12GB GPUメモリで余裕を持って動作可能です [2](https://apxml.com/posts/gemma-3-gpu-requirements)
- ：画像-テキスト処理に10.4GB VRAMを使用するため、12GB GPUメモリで問題なく動作します [2](https://apxml.com/posts/gemma-3-gpu-requirements)

これらのGemma 3モデルはGoogle DeepMindの最新マルチモーダルAIモデルで、テキストと画像の両方を処理できる能力を持っています [2](https://apxml.com/posts/gemma-3-gpu-requirements) 。

- 量子化バージョンを使用することで、12GB未満のVRAMでも動作可能 [4](https://github.com/WisconsinAIVision/ViP-LLaVA)
- マルチモーダルモデルで、画像理解能力を持っています
- ノート：「量子化ビットでの推論は、非量子化版ほど正確でない場合があります」と注意書きがあります [4](https://github.com/WisconsinAIVision/ViP-LLaVA)
- 12GB VRAMのGPU（RTX 3060など）では、13Bパラメータサイズのモデルも実行可能ですが [1](https://www.reddit.com/r/LocalLLaMA/comments/1765g70/from_no_gpu_to_a_3060_12gb_what_can_i_run/) 、画像処理を含むモデルではVRAM使用量が増加するためご注意ください
- 多くのモデルは量子化することでVRAM使用量を大幅に削減できますが、わずかな精度低下が発生する場合があります [2](https://apxml.com/posts/gemma-3-gpu-requirements)

12GB GPUメモリを持つRTX 3060はコストパフォーマンスに優れており [5](https://note.com/catap_art3d/n/n3796db26deab) 、適切に量子化されたモデルを使用すれば、上記のような画像理解能力を持つモデルを効率的に実行できます。

関連

12GB GPUで動作する他の画像理解LLMは

12GB GPUで最適な画像理解LLMの設定は

12GB GPUで画像理解LLMをトレーニングする方法は

12GB GPUで画像理解LLMのパフォーマンスを最大化するためのベストプラクティスは

12GB GPUで動作する画像理解LLMの具体的な例は